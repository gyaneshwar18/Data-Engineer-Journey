# Day 13 â€” OOP ETL Pipeline (Class-Based Data Job)

## ğŸ“Œ Objective
Build a class-based ETL (Extractâ€“Transformâ€“Load) pipeline using Python and Pandas.  
Instead of writing loose scripts, this project structures the data workflow using Object-Oriented Programming (OOP), similar to how production data pipelines are designed.

---

## ğŸ§  Concepts Covered

- Python Classes & OOP
- __init__ constructor
- Instance variables
- Method-based pipeline stages
- ETL design pattern
- Pandas aggregation
- CSV ingestion and export
- Modular pipeline structure

---

## âš™ï¸ ETL Stages Implemented

### 1ï¸âƒ£ Extract
- Read CSV file using Pandas
- Load into DataFrame

### 2ï¸âƒ£ Transform
- Convert marks column to numeric
- Handle invalid values
- Group by student name
- Compute average marks

### 3ï¸âƒ£ Load
- Save transformed output to CSV

### 4ï¸âƒ£ Run
- Orchestrates extract â†’ transform â†’ load sequence

---



## â–¶ï¸ How to Run

Install dependency:

    pip install pandas

Run pipeline:

    python etl_pipeline.py

---

## ğŸ“Š Sample Output (output.csv)

name,avg_marks
Aryan,80.0
Riya,89.0
Sneha,92.0

---

## ğŸ—ï¸ Why OOP for Data Pipelines?

Using classes makes pipelines:

- Reusable
- Configurable
- Testable
- Modular
- Production-friendly

Each ETL stage is a method:
- extract()
- transform()
- load()b 
- run()

This mirrors real-world pipeline frameworks like Airflow operators and ETL job classes.

---



