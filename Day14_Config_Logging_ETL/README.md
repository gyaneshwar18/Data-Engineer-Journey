# Configurable ETL Pipeline with Logging

## ğŸ“Œ Objective
Upgrade the class-based ETL pipeline by adding structured logging and external configuration.  
This makes the pipeline production-style, configurable, and observable â€” similar to real-world data engineering jobs.

This project demonstrates how to separate configuration from code and how to track pipeline execution using logs.

---

## ğŸ§  Concepts Covered

- Config-driven pipelines (JSON config file)
- Structured logging using Python logging module
- OOP ETL pipeline design
- Extractâ€“Transformâ€“Load stages
- Exception handling with logging
- Runtime configuration
- Pipeline observability

---

## âš™ï¸ Features Implemented

âœ… External config file (config.json)  
âœ… Structured logging to file  
âœ… Configurable input/output paths  
âœ… Logged ETL stages  
âœ… Error logging with stack trace  
âœ… OOP pipeline class  
âœ… Production-style run method  

---



## ğŸ“„ config.json

Contains runtime configuration instead of hardcoding values.






## ğŸ“ Log Output

After running, check:

    logs/etl.log

Example entries:

    INFO | Pipeline started
    INFO | Extract stage started
    INFO | Loaded 5 rows
    INFO | Transform stage started
    INFO | Aggregation complete
    INFO | Load stage started
    INFO | Saved output to output.csv
    INFO | Pipeline completed successfully

---

## ğŸ“Š Output (output.csv)

name,avg_marks  
Aryan,80.0  
Riya,89.0  
Sneha,92.0  

---

## ğŸ—ï¸ Why This Matters in Data Engineering

Production pipelines must be:

- Configurable (no hard-coded paths)
- Observable (logs for every run)
- Maintainable (modular design)
- Debuggable
